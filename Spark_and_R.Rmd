---
title: "Spark and R"
author: "Theo Boutaris"
date: "8 November 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Spark

Apache Spark is a fast and general engine for large-scale data processing as mentioned on the 
official [project's page](http://spark.apache.org/). For many it is considered to be the successor
of the popular Hadoop engine.

Spark has considerably increased in popularity in the last years since:

* It is easy to implement with existing technologies. It can run on:
    * Hadoop
    * Mesos
    * Standalone
    * In the cloud
* It can also access diverse data sources including:
    * HDFS
    * Cassandra
    * HBase 
    * S3
* It runs programs up to 100x faster than Hadoop MapReduce in RAM or 10x faster on disk.
* It is implemented in a plethora of languages like:
    * Java
    * Scala
    * Python
    * R

## R

R is a statistical programming language (also considered a general purpose language
after latest developments) and it is one of the languages used to run Spark for statistical / data
analysis.

In this course we will explore the two main R packages used to run data analysis on Spark:

* SparkR - natively included in Spark after version 1.6.2
* sparklyr - developed by RStudio

## Installation of Spark and Hadoop and versions

#### Versions and Spark Mode

I spent a considerable amount of time, trying to find out exactly which combination of Hadoop and 
spark would work with both SparkR and sparklyr (these are very new packages and there are still
some issues, especially with the recent update of Spark to version 2.0.1). According to my research
Spark version 2.0.1 (the most recent at the time of writing) and Hadoop version 2.4 seem to work
perfect with both SparkR and sparklyr. For the rest of the course I will be using this combination
of Spark and Hadoop.

I used the [standalone mode of Spark](http://spark.apache.org/docs/latest/spark-standalone.html) and
I am also working on Windows 10.

```{R}
#details of my session
sessionInfo()
```

#### Installation of Spark and Hadoop

The easiest way to install Spark Standalone and Hadoop is through `sparklyr::spark_install`:

```{R, eval = TRUE}
#install Spark 2.0.1 and Hadoop 2.4
#sparklyr version 0.4.22 
library(sparklyr)
spark_install(version = '2.0.1', hadoop_version = '2.4')
```

After the download and installation of Spark and Hadoop (this might take a while) we are ready to 
start using `SparkR` and `sparklyr`.

For clarity your spark and hadoop versions by default will be installed in:

`C:\Users\<your user>\AppData\Local\rstudio\spark\Cache\spark-2.0.1-bin-hadoop2.4`

## RStudio

`SparkR` is native to Spark since version 1.6.2 and can be immediately used through the command line.
`sparklyr` can be used after loading the package `sparklyr`. However, most users (including myself)
like working on an IDE (this is why we have IDEs, right?) like RStudio. In fact the [RStudio preview Release](https://www.rstudio.com/products/rstudio/download/preview/) includes a toolset for 
creating / managing spark connections, browsing Spark DataFrames and has specifically been 
designed to work with Spark.

For the rest of the course we will be using the RStudio preview release which can be downloaded for
free in the link above.

#### Installation of sparklyr and SparkR

`sparklyr` is readily available on CRAN so running `install.packages('sparklyr')` would be enough.
Things are slightly more complicated for `SparkR` since it has to be downloaded and installed from 
github. Nothing too difficult though:

```{R, eval = FALSE}
#you will need the devtools package to install packages from github
#so install.packages('devtools') if you don't have it
library(devtools)
install_github('apache/spark@v2.0.1', subdir='R/pkg')
```

<div style='background-color:lightgray;border-radius: 5px;border: 2px solid #73AD21;'>
Remember that working through the
command line SparkR can be done simply by visiting 
`C:\Users\<your user>\AppData\Local\rstudio\spark\Cache\spark-2.0.1-bin-hadoop2.4\bin\` and 
double clicking sparkR.exe or simple by running sparkR on windows cmd if the above path is 
part of the environment variable `%path%`.
</div>
     
And that would be enough to have both sparklyr and SparkR.

## SparkR

Our first task would be to connect RStudio to spark using SparkR.

To do this we need first need to set the `SPARK_HOME` environment variable (don't worry we will 
do it through R and it will be super easy) and then connect to spark through `spark.session`:

```{R}
#lots of base R functions will be masked so please use SparkR only for working with Spark
#remember to use your own username instead of teoboot which is mine
#this sets the SPARK_HOME environment variable
Sys.setenv(SPARK_HOME = "C:/Users/teoboot/AppData/Local/rstudio/spark/Cache/spark-2.0.1-bin-hadoop2.4")

#SparkR version 2.0.1
library(SparkR)

#start a spark R session in SparkR.
#by not setting the SPARK_HOME environment variable sparkR.session will attempt to download it
sc1 <- sparkR.session()
```

For the following analysis we will be using the popular `flights` from the `nycflights13` 
package.

Firstly, typical data frame operations in local R would involve subseting columns, rows, grouping
and aggregating. Let's see how we can do this in SparkR:

```{R}
#load the package in order to use flights
library(nycflights13)
```

In order to create a spark dataframe we use `createDataFrame`:

```{R}
df <- createDataFrame(flights)
```


In order to subset columns from flights we use `select`:

```{R}
head(select(df, df$air_time, df$distance)) 
```

Another way would be to use the `[` operator as we would for a local dataframe:

```{R}
head(df[, c('air_time', 'distance')])
```

In order to subset rows we use `filter`:

```{R}
head(filter(df, df$distance > 3000))
```

Another way would be to use the `[` operator as we would in a local dataframe
but please note that subseting the rows of a data.frame with indices would not work
i.e. `df[1:100], ]` would not work.

```{R}
head(df[df$distance > 3000, ])
```

In order to group and aggregate we use `summarize` and `groupBY`:

```{R}
grouped_df <- groupBy(df, df$origin)
df2 <- summarize(grouped_df, 
                 mean = mean(df$distance), 
                 count = n(df$origin),
                 sum = sum(df$distance))
head(df2)
```

In order to sort the data frame we use `arrange`:

```{R}
#head(arrange(df2, desc(df2$mean))) for sorting in descending order or
head(arrange(df2, df2$mean)) 
```

However, the above processes can be easily combined with the pipe operator `%>%` from the 
`magrittr` package. In this way we can work in a similar way to `dplyr`, `ggvis`, `tableHTML` or
other. The above process would be written like this:

```{R}
library(magrittr)
df3 <- df %>%
        group_by(df$origin)%>%
        summarize(mean = mean(df$distance), 
                  count = n(df$origin),
                  sum = sum(df$distance)) 

arrange(df3, df3$mean) %>% 
 head
```

And as you can see we get the same results.

SparkR also supports the use of SQL commands by registering a table as a sql table:

```{R}
#create a sql table
createOrReplaceTempView(df, "sql_df")

#query sql table using sql syntax
df4 <- sql("SELECT * FROM sql_df WHERE distance > 4800")
head(df4)
```

There will be cases where we would want to collect our data sets from spark and work on them 
locally. This can be done using `collect`.

```{R}
#local_df will be a data frame on our local R instance
local_df <- collect(df3)
local_df[1:2, ]
```

#### Machine Learning - SparkR

It is essential for our work to be able to do machine learning on a data set. SparkR offers this
capability through MLlib.

By continuing to use the `flights` data set we will predict distance.

First of all we need to split our data set into a train and a test set with the `randomSplit` 
function. 

```{R}
#split into train and test - 20% for test and 80% for train
df_list <- randomSplit(df, c(20, 80))

#test
test <- df_list[[1]]
nrow(test)

#train
train <- df_list[[2]]
nrow(train)

#validation - nrow(test) + nrow(train) == nrow(df)
nrow(df)
```

Now that we have our train and test sets we are ready to run our linear model. We will train our 
model on the train set and then predict on our test set. We will try to predict distance based on 
air_time for this simple example.

Notice that `glm` cannot handle NAs that might exist within the DataFrame which will make the 
function crash with an uninformative error. Make sure you remove NAs beforehand with `na.omit`. 

```{R}
#run model
train <- na.omit(train)
my_mod <- glm(distance ~ air_time, data = train, family = 'gaussian')
```

Let's check the model coefficients:

```{R}
#summary
summary(my_mod)
```

Now we need to make the predictions on our test set in order to assess the goodness of fit.
`glm` or SparkR do not provide any metrics or functions to assess the goodness of fit. These need to
be created by the user like the calculation of the MSE below: 

```{R}
#predict
#omit the NAs manually here otherwise spark will crash without
#an informative error
test <- na.omit(test)
preds <- predict(my_mod, newData = test)

#predictions and actual distance
head(select(preds, 'distance', 'prediction'))

#add squared residuals using transform
sq_resid <- transform(preds, sq_residuals = (preds$distance - preds$prediction)^2)

#calculate MSE and collect locally - it is only a number
MSE <- collect(summarize(sq_resid, mean = mean(sq_resid$sq_residuals)))$mean

#RMSE
sqrt(MSE) 
```

Lastly, `SparkR` seems to offer just four machine learning algorithms at the time of writting:

* Generalised Linear Models 
* kmeans
* Naive Bayes
* Survival Regression Model

## Sparklyr

```{R}
library(sparklyr)
library(dplyr)
#make a connection
sc2 <- spark_connect('local', version = '2.0.1', hadoop_version = '2.4', config = list())
```

Notice, that when loading `sparklyr` it will mask many base (or SparkR functions if you have 
SparkR loaded) functions.

Now we will try to do the same process using the sparklyr package. 

In sparklyr `copy_to` will transfer the flights data set to our clusters in the same way that 
`createDataFrame` did in SparkR.

```{R}
#load nycflights13 for flights data set
library(nycflights13)

#pass to spark
df <- copy_to(sc2, flights)
```

`sparklyr` uses `dplyr` for all data manipulation processes using the usual `dplyr` verbs.

In order to select specific columns we use `sparklyr::select` in the same way we would in `dplyr`:

```{R}
head(select(df, distance, origin))
```

Notice that unlike SparkR, `df[, 'origin']` would not work on `sparklyr`.

In a similar way `filter` is used to subset rows from df:

```{R}
head(filter(df, arr_delay > 30))
```

Notice again that unlike SparkR, `df[distance > 3000, ]` would not work on `sparklyr`.

Also, for sparklyr (as with dplyr) only the column name is required (unquoted) for subsetting df, 
whereas for SparkR we use `df$column_name`. To see the difference clearly, I add the two below:

```{R, eval = FALSE}
SparkR::filter(df, df$distance > 3000)
sparklyr::filter(df, distance > 3000)
```

Grouping and aggregation in `sparklyr` is done in the exact same way as R users would do it in 
`dplyr` locally. `dplyr` promotes (and includes) the use of the pipe operator from `magrittr` 
(`%>%`). Therefore, even though we can perform a step by step group and aggregation process 
(remember we started like this with SparkR before we started using the pipe operator as well) we 
will use directly the chaining method (i.e. using pipe) which is what `dplyr` users are familiar 
with anyway.

```{R}
#group and summurise
df_aggr <- 
  df %>%
   group_by(origin) %>%
   summarise(mean = mean(distance),
             count = n(),
             sum = sum(distance))
head(df_aggr)
```

You can see that in terms of selecting columns, subsetting rows, grouping and aggregating, SparkR 
and sparklyr have many similarities. Notice again that for sparklyr we only specify column names 
instead of the full `df$column_name` syntax. Also, function `n` does not take any arguments. The
results are consistent between the two processes.

In a similar way, in order to sort df we will still use `dplyr::arrange`:

```{R}
arrange(df_aggr, desc(mean)) %>% 
  head
```

`sparklyr` also supports sql in the same way as SparkR using the function `dbGetQuery`. `dbGetQuery`
comes from the DBI packge.

```{R}
library(DBI)
df_sql <- dbGetQuery(sc2, "SELECT * FROM flights WHERE distance > 4800 LIMIT 5")
df_sql
```

Notice that `dbGetQuery` is used on the name of the object `copy_to` was applied on i.e. `flights` 
and not `df`. On RStudio under the Environment tab you should be able to see the flights data set.
The data sources you see there are the ones you can actually access with sql.

Again in the same way as SparkR, `dplyr::collect` will bring a df back from spark to a local 
dataframe.

```{R}
local_df <- collect(df_aggr)
local_df[1:2, ]
```

## Machine Learning - sparklyr

It is `sparklyr`'s turn to see how we can use its functions to do machine learning. We didn't see
many differences between the two packages in terms of data manipulation but we will see considerable
differences in machine learning.

Let's start again by splitting our data set into a train and a test set in the same way that we did
with SparkR. To do this we use `sdf_partition`:

```{R}
partitions <- sdf_partition(df, training = 0.8, test = 0.2, seed = 1099)
count(df)

train <- partitions$training
count(train)

test <- partitions$test
count(test)
```

`dplyr` does not have an `nrow` function but we can easily use `count` to make sure that our data set
was split properly.

Unlike `SparkR`, `sparklyr` offers a plethora of models or machine learning techniques to choose 
from including:

* ml_als_factorization                   
* ml_binary_classification_eval         
* ml_classification_eval                 
* ml_create_dummy_variables             
* ml_decision_tree                       
* ml_generalized_linear_regression      
* ml_gradient_boosted_trees              
* ml_kmeans                             
* ml_lda                                 
* ml_linear_regression                  
* ml_load                                
* ml_logistic_regression                
* ml_model                               
* ml_multilayer_perceptron              
* ml_naive_bayes                         
* ml_one_vs_rest                        
* ml_options                             
* ml_pca                                
* ml_prepare_dataframe                   
* ml_prepare_features                   
* ml_prepare_response_features_intercept 
* ml_random_forest                      
* ml_save                                
* ml_survival_regression                
* ml_tree_feature_importance            

Now that we have our training and test sets we are ready to run our regression model. 

```{R}
fit <- ml_linear_regression(train, response = "distance", features = "air_time")
fit

#summary 
summary(fit)
```

`ml_linear_regression` seems to be a better alternative to `SparkR::glm` since it can handle NAs 
without having to run `na.omit` before training the model (SparkR does not return a meaningful
error either in case you forget to remove NAs). It also provides an R squared and RMSE by default
which gives a first idea about the goodness of fit.

Of course, although `ml_linear_regression` provides an R squared and an RMSE we need to check those
on the test set in order to assess the goodness of fit properly. We will do this in a similar way 
as we did in SparkR.

We will use `na.omit` in this case because we know some values in `distance` have missing values and 
we want to make sure predictions are correct. Also, make sure to use `sdf_predict` instead of 
`predict` because the latter will also collect and return an atomic vector. 

`mutate` is used instead of `transform` in `sparklyr` to add a column.

```{R}
#add squared residuals using transform
test <- na.omit(test)

#calculate predictions - sdf_predict will add predictions as a column
preds <- sdf_predict(fit, data = test)

#add squared residuals in preds
sq_resid <- mutate(preds, sq_residuals = (distance - prediction)^2)

#calculate MSE and collect locally
MSE <- collect(summarise(sq_resid, mean = mean(sq_residuals)))$mean

#RMSE
sqrt(MSE) 
```

## Notes

Having used both SparkR and spalklyr in the above course I can say that I found sparklyr to be 
easier to work with (this is only a preference) and actually considerably faster (although this 
is on standalone mode) in comparison to SparkR. Also, I used spaklyr to install Hadoop and Spark 
which might also be a reason for the speed difference.

However, the fact is that spakrlyr in terms of data manipulation uses the exact same verbs as dplyr
(which is a very popular package) so the learning curve might be easier. In terms of machine 
learning sparklyr is unquestionably more advanced since it has implemented a lot more algorithms 
compared to SparkR and they have done a great job making the functions very easy to use. 

Also, sparklyr's functions are documented in R's help (i.e. using `?function_name` will work), as 
opposed to SparkR which doesn't have its functions documented in R at the time of writing (remember
it was downloaded from github and not CRAN). Documentation can be found online though.

## H2O

sparklyr is actually compatible with H2O. Unfortunately, it is only compatible with Spark 1.6 
because Spark 2.0 just came out. It is in the pipeline to make it compatible with Spark 2.0.
If you are using spark 1.6 and want to find out more about sparklyr and H2O you can have a look at 
this [link](http://spark.rstudio.com/h2o.html).

## Github

The code for this course is hosted on [github](https://github.com/LyzandeR/Rstudio-Spark-Tutorial).

Thanks for reading!

